{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa985fa-bcda-49ed-944a-c12dc6c22c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067184cd53b847e8af381d3c05840c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mMLP-KTLim/llama-3-Korean-Bllossom-8B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m model.eval()\n\u001b[32m     14\u001b[39m PROMPT = \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful AI assistant. Please answer the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:374\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    373\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/modeling_utils.py:4063\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4046\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4047\u001b[39m load_config = LoadStateDictConfig(\n\u001b[32m   4048\u001b[39m     pretrained_model_name_or_path=pretrained_model_name_or_path,\n\u001b[32m   4049\u001b[39m     ignore_mismatched_sizes=ignore_mismatched_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4061\u001b[39m     download_kwargs=download_kwargs,\n\u001b[32m   4062\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m4063\u001b[39m loading_info, disk_offload_index = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4064\u001b[39m loading_info = \u001b[38;5;28mcls\u001b[39m._finalize_model_loading(model, load_config, loading_info)\n\u001b[32m   4065\u001b[39m model.eval()  \u001b[38;5;66;03m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/modeling_utils.py:4182\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(model, state_dict, checkpoint_files, load_config)\u001b[39m\n\u001b[32m   4179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeither a state dict nor checkpoint files were found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4182\u001b[39m loading_info, disk_offload_index = \u001b[43mconvert_and_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtp_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tp_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4190\u001b[39m \u001b[38;5;66;03m# finally close all opened file pointers\u001b[39;00m\n\u001b[32m   4191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_pointer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/core_model_loading.py:1192\u001b[39m, in \u001b[36mconvert_and_load_state_dict_in_model\u001b[39m\u001b[34m(model, state_dict, load_config, tp_plan, disk_offload_index)\u001b[39m\n\u001b[32m   1190\u001b[39m pbar.refresh()\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     realized_value = \u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m target_name, param \u001b[38;5;129;01min\u001b[39;00m realized_value.items():\n\u001b[32m   1200\u001b[39m         param = param[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m param\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/core_model_loading.py:665\u001b[39m, in \u001b[36mWeightRenaming.convert\u001b[39m\u001b[34m(self, layer_name, model, config, hf_quantizer, loading_info)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    657\u001b[39m     layer_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[39;00m\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# attribute during the whole process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     collected_tensors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaterialize_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m     \u001b[38;5;66;03m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[39;00m\n\u001b[32m    668\u001b[39m     \u001b[38;5;66;03m# only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\u001b[39;00m\n\u001b[32m    669\u001b[39m     target_key = \u001b[38;5;28mself\u001b[39m.target_patterns[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/site-packages/transformers/core_model_loading.py:641\u001b[39m, in \u001b[36mWeightTransform.materialize_tensors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# Async loading\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors[\u001b[32m0\u001b[39m], Future):\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     tensors = [future.result() \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# Sync loading\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(tensors[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/aiffel_learning_py312/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "PROMPT = \"You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.\"\n",
    "instruction = \"서울의 유명한 관광 코스를 만들어줄래?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": PROMPT},\n",
    "    {\"role\": \"user\", \"content\": instruction},\n",
    "]\n",
    "\n",
    "enc = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "enc = enc.to(model.device)\n",
    "\n",
    "eot = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "terminators = [tokenizer.eos_token_id] + ([eot] if eot is not None else [])\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc.get(\"attention_mask\", None),\n",
    "    max_new_tokens=512,          # 처음엔 2048 말고 256~512 추천\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "prompt_len = enc[\"input_ids\"].shape[-1]\n",
    "print(tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389da3ae-3914-4dcd-8ebd-a74ac819de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textAugmentation(text):\n",
    "    PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly.\n",
    "                당신은 문장을 재구성하는 전문가입니다. 주어진 문장과 의미가 동일한 새로운 문장을 생성해주세요.'''\n",
    "    instruction = f\"아래 문장과 동일한 의미를 갖는 새로운 문장을 하나만 만들어줘:\\n\\n'{text}'\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "    \n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc = enc.to(model.device)\n",
    "    \n",
    "    eot = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    terminators = [tokenizer.eos_token_id] + ([eot] if eot is not None else [])\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc.get(\"attention_mask\", None),\n",
    "        max_new_tokens=512,          # 처음엔 2048 말고 256~512 추천\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    prompt_len = enc[\"input_ids\"].shape[-1]\n",
    "\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593e35d4-99d7-4bb2-9d74-5e4e683143d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"길동경찰서입니다. 9시 40분 마트에 폭발물을 설치할거다. 네? 똑바로 들어 한번만 더 얘기한다. 장난전화 걸지 마시죠. 9시 40분 마트에 폭발물이 터지면 다 죽는거야. 장난전화는 업무방해죄에 해당됩니다. 판단은 너에게 달려있다. 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지. 선생님 진정하세요. 난 이야기했어. 경고했다는 말이야.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8247fb7-1869-47e4-8bf8-f1209fccec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길동경찰서입니다. 9시 40분 마트에 폭발물을 설치할 거예요. 네? 똑바로 들어 한번만 더 얘기하겠습니다. 장난전화는 업무 방해죄에 해당됩니다. 9시 40분 마트에 폭발물이 터지면 모두 죽을 것입니다. 판단은 당신에게 달려 있습니다. 길동경찰서에도 폭발물 터지면 꽤 재미있겠지요. 선생님, 진정하세요. 이야기를 했어요. 경고를 했으니 주의하세요.\n"
     ]
    }
   ],
   "source": [
    "print(textAugmentation(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8f3497-f162-495f-9c90-ee7db18e9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"어이 거기 예?? 너 말이야 너. 이리 오라고 무슨 일. 너 옷 좋아보인다? 얘 돈 좀 있나봐 아니에요.돈 없어요 뒤져서 나오면 넌 죽는다 오늘 피시방 콜? 콜. 마지막 기회다. 있는거 다 내놔 정말 없어요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a8b5c0-8878-4211-98ef-a53dd78818e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'어이 거기 예?? 너 말이야 너. 이리 오라고 무슨 일. 너 옷 좋아보인다? 얘 돈 좀 있나봐 아니에요. 돈 없어요 뒤져서 나오면 넌 죽는다 오늘 피시방 콜? 콜. 마지막 기회다. 있는거 다 내놔 정말 없어요.'\n"
     ]
    }
   ],
   "source": [
    "print(textAugmentation(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac5a6a6b-8515-476e-86ee-71e245314c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch cuda version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"torch cuda version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07a517c7-24f9-4409-869a-39b24d050206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b836b1-3cbc-41c5-877c-f0c999f17f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f8f2d2914349b49ebd2f6fe71c6baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cuda:0\n",
      "철수가 20개의 연필을 가지고 있었고, 영희가 절반을 가져갔으니, 영희가 가져간 연필은 20 / 2 = 10개입니다.\n",
      "\n",
      "철수가 남은 연필은 20 - 10 = 10개입니다.\n",
      "\n",
      "민수가 남은 5개를 가져갔으니, 철수가 남은 연필은 10 - 5 = 5개입니다.\n",
      "\n",
      "따라서 철수가 남은 연필의 갯수는 5개입니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "instruction = \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "enc = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "print(\"model device:\", next(model.parameters()).device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=1024,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9\n",
    "# )\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=enc[\"input_ids\"],\n",
    "    attention_mask=enc.get(\"attention_mask\", None),\n",
    "    max_new_tokens=512,          # 처음엔 2048 말고 256~512 추천\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "\n",
    "prompt_len = enc[\"input_ids\"].shape[-1]\n",
    "print(tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True))\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "# print(tokenizer.decode(outputs[0][input_ids.shape[-1]:]))\n",
    "# print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d57a2a-c33d-4e16-aa15-c93a3c5b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textAugmentation(model, PROMPT, instruction):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc.get(\"attention_mask\", None),\n",
    "        max_new_tokens=512,          # 처음엔 2048 말고 256~512 추천\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    prompt_len = enc[\"input_ids\"].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54d37727-1350-4e38-972f-008bb09c2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요! 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네\n"
     ]
    }
   ],
   "source": [
    "text = \"저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요! 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네\"\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "    너는 한국어 대화 데이터 증강 전문가야.\n",
    "\n",
    "    규칙\n",
    "    1. 자연스러운 한국어로 생성\n",
    "    \"\"\"\n",
    "instruction = f\"의미를 유지해서 문장의 단어와 말투를 바꿔라.:\\n\\n'{text}'\"\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0852bc52-6e02-422b-9480-0fd0e780de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요. 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네.'\n",
      "⏱ GPU 포함 실행 시간: 8.494초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "print(textAugmentation(model, PROMPT, instruction))\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b5e05e7-036c-4870-9df6-9611adedcf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김 대리 승진해야지 부장님 그래도 이건 아닙니다. 사는게 다 그래. 나는 이렇게 안 한줄 알아? 그래도 전 제 소신을 지키고 싶습니다. 소신은 나가서 지켜요 여기 말고요 부장님. 난 말했고. 김 대리가 부처할거면 해. 근데 여긴 그런 곳 아냐 어쩌면 좋죠. 그걸 알면 내가 알려줘? 알아? 그냥 닥치는대로 해 알겠습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"김 대리 승진해야지 부장님 그래도 이건 아닙니다. 사는게 다 그래. 나는 이렇게 안 한줄 알아? 그래도 전 제 소신을 지키고 싶습니다. 소신은 나가서 지켜요 여기 말고요 부장님. 난 말했고. 김 대리가 부처할거면 해. 근데 여긴 그런 곳 아냐 어쩌면 좋죠. 그걸 알면 내가 알려줘? 알아? 그냥 닥치는대로 해 알겠습니다.\"\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "    너는 한국어 대화 데이터 증강 전문가야.\n",
    "\n",
    "    규칙\n",
    "    1. 자연스러운 한국어로 생성\n",
    "    \"\"\"\n",
    "instruction = f\"의미를 유지해서 문장 길이를 줄여라.:\\n\\n'{text}'\"\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f46e9aa-f93f-4a3f-ab9c-9f903f5b2a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김 대리가 승진하면 좋겠지만, 여기서 내 소신을 지키고 싶습니다. 소신은 여기서 말고요. 김 대리가 부처할 때 해. 여기서 닥치는대로 해. 알겠습니다.'\n",
      "⏱ GPU 포함 실행 시간: 2.205초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "print(textAugmentation(model, PROMPT, instruction))\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0ea901a-5170-4b3d-8971-4b6d901b77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요! 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네\n"
     ]
    }
   ],
   "source": [
    "text = \"저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요? 아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요! 여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요. 음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요. 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요. 네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요. 아니 저기요 돈 안내요? 네? 발라보는것도 돈 내야 하나요? 그럼 이거 누구한테 팔아요? 당신이 바른거를? 아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요? 내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지 진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네\"\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "    넌 영어 번역가야. 한국어를 영어로 번역해줘\n",
    "\n",
    "    규칙\n",
    "    1. 설명하지 말고 답변만 출력할 것\n",
    "    2. 화자 간 관계는 바꾸지 말 것\n",
    "    \"\"\"\n",
    "instruction = f\"다음 대화 문장을 영어로 번역해줘. 출력은 따로 설명 없이 번역문만 출력해줘:\\n\\n'{text}'\"\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a6c0526-ef00-4466-93f1-58053ab2af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ GPU 포함 실행 시간: 6.557초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "kor_to_eng_text = textAugmentation(model, PROMPT, instruction)\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c34af79-5eb8-4ad7-8125-9facd2c8fd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it too hot here? Do you want to try some sunscreen from our company? Really? I was just looking for some sunscreen anyway. Here, try it out. It's really good, and it's great for everyone. I mean, it's good because you said so, but it might not be good for your skin. It stings a bit. This one is really well-researched, but I guess you're sensitive. You are very sensitive. So, buy a lot of it, and goodbye. Are you okay? Do you need some money? Do you need to buy it? So, who are you going to sell this to? Are you going to sell the right one? You said to try it out, not to buy it. I don't understand why you're taking advantage of people like this. I never said I wouldn't buy it. You didn't know that, did you? It's not your fault. You should sell it yourself, not take advantage of others. It's really ridiculous.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "    넌 한국어 번역가야. 영어를 한국어로 번역해줘\n",
    "    \"\"\"\n",
    "instruction = f\"다음 대화 문장을 한국어로 번역해줘:\\n\\n'{kor_to_eng_text}'\"\n",
    "\n",
    "\n",
    "print(kor_to_eng_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6fade0c-75f4-431e-83e5-abe17ab659aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ GPU 포함 실행 시간: 6.972초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "eng_to_kor_text = textAugmentation(model, PROMPT, instruction)\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cb373ba-c867-468d-8dd1-184cfe530919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'이곳이 너무 따뜻하네요? 우리 회사의 солнscreen을 시도해 보세요?จริง으로? 나도 그냥 солнscreen을 찾고 있었습니다. 여기서 thử해 보세요. 정말 좋아요, 그리고 mọi人的 좋아요에 좋습니다. 말해도 좋지만, 당신의 피부에 좋지 않을 수 있습니다. 조금 쾌라지게 느껴집니다. 이 제품은 정말 잘 연구된 것 같지만, 당신이 너무 민감한 것 같아요. 당신이 매우 민감한 것 같아요. 그래서 많은 수량을 구입해 주세요, 그리고 goodbye. 당신이 okay하세요? 당신이 돈이 필요해? 당신이 구입해야 할 것인가? 이 제품을 whom에게 판매할 건가요? 이 제품을 올바른 제품으로 판매할 건가요? 당신이 시도해 보라고 말했어, 구입하지 말라고. 왜 사람들을 이 तरह 유혹하는 건지 이해 못합니다. 나는 구입하지 않겠다고 말한 건 아니잖아요. 당신은 그것을 알고 있지 않았어. 그것은 당신의 책임이 아니야. 당신이 자신이 판매해야 할 것 같아, 다른 사람들을 유혹하는 것이 아니야. 정말 무리스럽다.'\n"
     ]
    }
   ],
   "source": [
    "print(eng_to_kor_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e8cb4bf-90db-4697-bb44-6ba19c20593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "    너는 한국어 대화 데이터 증강 전문가야.\n",
    "\n",
    "    규칙\n",
    "    1. 대화는 총 4번 오고가게 작성.\n",
    "    2. 자연스러운 한국어로 생성\n",
    "    3. 설명 없이 요청한 답만 출력\n",
    "    4. 화자 구분 없이 이어서 작성\n",
    "\"\"\"\n",
    "instruction = f\"\"\"\n",
    "    두 사람이 대화하는 대화문을 하나 작성해줘.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cbeadd10-25f8-4d8e-a77f-3bf08e421ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이민호: 안녕하세요, 오늘은 무슨 계획이 있어요?\n",
      "정미: 오늘은 친구들과 영화를 보러 가고요. 영화가 나중에 나와서 시간을 맞추게 해요.\n",
      "이민호: 좋아요. 그럼 그 영화는 어떤 것인가요?\n",
      "정미: 요즘에 많이 보이던 '아프리카'입니다. 그게 재미있을 거예요.\n",
      "⏱ GPU 포함 실행 시간: 2.649초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "print(textAugmentation(model, PROMPT, instruction))\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9f4dac-c860-4b69-89ce-0e272897c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "    너는 두 사람이 일상적으로 대화하는 문장을 만들어주는 AI야.\n",
    "\"\"\"\n",
    "\n",
    "topic = \"부모님과의 여행\"\n",
    "\n",
    "instruction = f\"\"\"\n",
    "    대화하는 문장을 만들어줘.\n",
    "    각 문장은 한 줄로 작성해.\n",
    "    화자 이름은 쓰지 마.\n",
    "    자연스럽고 현실적인 말투로 대화해.\n",
    "    \n",
    "    답변예시1\n",
    "    이번 주말에 뭐 할까?\n",
    "    영화 한 편 보는 건 어때?\n",
    "    좋다, 팝콘도 사 가자.\n",
    "    그럼 내가 예매할게.\n",
    "    \n",
    "    답변예시2\n",
    "    오늘 회사에서 너무 피곤했어.\n",
    "    무슨 일 있었어?\n",
    "    회의가 계속 있어서 힘들었어.\n",
    "    오늘은 일찍 쉬어야겠다.\n",
    "    \n",
    "    답변예시3\n",
    "    요즘 어디 여행 가고 싶어?\n",
    "    바다 있는 곳이 좋을 것 같아.\n",
    "    부산이나 제주도 괜찮겠다.\n",
    "    사진도 많이 찍자.\n",
    "\n",
    "    \n",
    "    아래 {topic} 주제로 비슷한 형식의 대화 한 개만 만들어.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e19310-919a-4522-803f-56fcd738ea96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m torch.cuda.synchronize()   \u001b[38;5;66;03m# ✅ GPU 대기\u001b[39;00m\n\u001b[32m      5\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(textAugmentation(\u001b[43mmodel\u001b[49m, PROMPT, instruction))\n\u001b[32m      9\u001b[39m torch.cuda.synchronize()   \u001b[38;5;66;03m# ✅ 다시 대기\u001b[39;00m\n\u001b[32m     11\u001b[39m end_time = time.time()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ GPU 대기\n",
    "start_time = time.time()\n",
    "\n",
    "print(textAugmentation(model, PROMPT, instruction))\n",
    "\n",
    "torch.cuda.synchronize()   # ✅ 다시 대기\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"⏱ GPU 포함 실행 시간: {elapsed:.3f}초\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiffel_learning_py312)",
   "language": "python",
   "name": "aiffel_learning_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
